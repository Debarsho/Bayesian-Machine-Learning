# -*- coding: utf-8 -*-
"""PML_BayesianLR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZsBXAOIPJ-yGTXfgR1XuSwOpp4mxEZM0
"""

# Commented out IPython magic to ensure Python compatibility.
# Pandas and numpy for data manipulation
import pandas as pd
import numpy as np
np.random.seed(42)

 
# Matplotlib and seaborn for plotting
import matplotlib.pyplot as plt
# %matplotlib inline

import matplotlib
matplotlib.rcParams['font.size'] = 16
matplotlib.rcParams['figure.figsize'] = (9, 9)

import seaborn as sns

from IPython.core.pylabtools import figsize

# Scipy helper functions
from scipy.stats import percentileofscore
from scipy import stats

# Distributions
import scipy

# PyMC3 for Bayesian Inference
import pymc3 as pm

data=pd.read_csv('train.csv')
data.head()

data.shape

data.describe()

# Print the value counts for categorical columns
for col in data.columns:
    if data[col].dtype=='object':
        print('\nColumn Name:', col)
        print(data[col].value_counts())
# No output ensures that there are no categorial columns

# Our target variable is medv
# Checking correlations of other features with 'medv' for effective feature selection
data.corr()['medv'].sort_values()

# Selecting only top 6 correleted features namely lstat,ptratio,indus,rm,zn,black and dropping other columns.
xtrain = data.drop(columns=['tax', 'nox', 'crim', 'age','rad','ID','chas','dis'])
xtrain.head()

xtrain.shape

cmap = sns.cubehelix_palette(light=1, dark = 0.1,
                             hue = 0.5, as_cmap=True)

sns.set_context(font_scale=2)

# Pair grid set up
g = sns.PairGrid(xtrain)

# Scatter plot on the upper triangle
g.map_upper(plt.scatter, s=10, color = 'red')

# Distribution on the diagonal
g.map_diag(sns.distplot, kde=False, color = 'red')

# Density Plot on the lower triangle
g.map_lower(sns.kdeplot, cmap = cmap)

# On the diagonal, we have histograms showing the distribution of a single variable.
# The lower right traingle has both 2-D density plots.
# On the upper triangle, we have scatterplots of every variable plotted against one another.

xtrain.describe()

def evaluate_predictions(predictions,true):
    mae=np.mean(abs(predictions-true)) # mean absolute error
    rmse=np.sqrt(np.mean((predictions - true) ** 2)) # root mean squared error
    
    return mae,rmse

testing=pd.read_csv('test.csv')
testing.head()

xtest=testing.drop(columns=['tax', 'nox', 'crim', 'age','rad','ID','chas','dis'])
xtest.head()
xtest.shape

xtest.head()

xtrain.head()

# Implementing ordinary least squared linear regresion
xtrain1=xtrain.drop(columns=['medv'])
xtrain1['x0']=1
X=xtrain1.values
ytrain1=xtrain['medv']
Y=ytrain1.values
X1=X.transpose()
A=np.matmul(X1,X)
A=np.linalg.inv(A)
B=np.matmul(X1,Y)
weights=np.matmul(A,B)

predicted=np.matmul(X,weights)

#plotting residual errors
## setting plot style 
plt.style.use('fivethirtyeight') 
  
## plotting residual errors in training data 
plt.scatter(predicted, predicted - Y, 
            color = "green", s = 10, label = 'Train data') 
   
  

## plotting line for zero residual error 
plt.hlines(y = 0, xmin = 0, xmax = 50, linewidth = 2) 
  
## plotting legend 
plt.legend(loc = 'upper right') 
  
## plot title 
plt.title("Residual errors") 
  
## function to show plot 
plt.show()

# Formula for Bayesian Linear Regression (follows R formula syntax)
# In this syntax, ~, is read as “is a function of”. 
# We are telling the model that Grade is a linear combination of the six features on the right side of the tilde.
formula = 'medv ~ ' + ' + '.join(['%s' % variable for variable in xtrain.columns[0:6:]])
formula

# We now build the model using the formula defined above and a normal distribution for the data likelihood.
# Then, we let a Markov Chain Monte Carlo algorithm draw samples from the posterior to approximate the posterior for each of the model parameters.
with pm.Model() as normal_model:
    
    # The prior for the model parameters will be a normal distribution
    family = pm.glm.families.Normal()
    
    # Creating the model requires a formula and data (and optionally a family)
    pm.GLM.from_formula(formula, data = xtrain, family = family)
    
    # Perform Markov Chain Monte Carlo sampling
    normal_trace = pm.sample(draws=2000, chains = 2, tune = 500)

# Shows the trace with a vertical line at the mean of the trace
def plot_trace(trace):
    # Traceplot with vertical lines at the mean value
    ax = pm.traceplot(trace, figsize=(14, len(trace.varnames)*1.8),
                      lines={k: v['mean'] for k, v in pm.df_summary(trace).iterrows()})
    
    matplotlib.rcParams['font.size'] = 16
    
    # Labels with the median value
    for i, mn in enumerate(pm.df_summary(trace)['mean']):
        ax[i, 0].annotate('{:0.2f}'.format(mn), xy = (mn, 0), xycoords = 'data', size = 8,
                          xytext = (-18, 18), textcoords = 'offset points', rotation = 90,
                          va = 'bottom', fontsize = 'large', color = 'red')

pm.plot_posterior(normal_trace, figsize = (14, 14), text_size=20);
# prosterior distribution

# Linear Formula from Bayesian Inference using Mean of Parameters
model_formula = 'Grade = '
for variable in normal_trace.varnames:
    model_formula += ' %0.2f * %s +' % (np.mean(normal_trace[variable]), variable)

' '.join(model_formula.split(' ')[:-1])

# Examines the effect of changing a single variable
# Takes in the name of the variable, the trace, and the data
def model_effect(query_var, trace, X):
    
    # Variables that do not change
    steady_vars = list(X.columns)
    steady_vars.remove(query_var)
    
    # Linear Model that estimates a grade based on the value of the query variable 
    # and one sample from the trace
    def lm(value, sample):
        
        # Prediction is the estimate given a value of the query variable
        prediction = sample['Intercept'] + sample[query_var] * value
        
        # Each non-query variable is assumed to be at the median value
        for var in steady_vars:
            
            # Multiply the weight by the median value of the variable
            prediction += sample[var] * X[var].median()
        
        return prediction
    
    figsize(6, 6)
    
    # Find the minimum and maximum values for the range of the query var
    var_min = X[query_var].min()
    var_max = X[query_var].max()
    
    # Plot the estimated grade versus the range of query variable
    pm.plot_posterior_predictive_glm(trace, eval=np.linspace(var_min, var_max, 100), 
                                     lm=lm, samples=100, color='blue', 
                                     alpha = 0.4, lw = 2)
    
    # Plot formatting
    plt.xlabel('%s' % query_var, size = 16)
    plt.ylabel('medv', size = 16)
    plt.title("Posterior of medv vs %s" % query_var, size = 18)
    plt.show()

model_effect('zn', normal_trace, xtrain.drop(columns='medv'))

model_effect('indus', normal_trace, xtrain.drop(columns='medv'))

model_effect('rm', normal_trace, xtrain.drop(columns='medv'))

model_effect('ptratio', normal_trace, xtrain.drop(columns='medv'))

model_effect('black', normal_trace, xtrain.drop(columns='medv'))

model_effect('lstat', normal_trace, xtrain.drop(columns='medv'))

